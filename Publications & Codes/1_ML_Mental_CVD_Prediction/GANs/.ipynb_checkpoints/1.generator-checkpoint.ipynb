{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8141cd45-8358-4ddf-a8b3-724ab6eb0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "path = \"/groups/umcg-lifelines/tmp02/projects/ov20_0110/Lifelines\"\n",
    "\n",
    "# # List contents\n",
    "# print(os.listdir(path))\n",
    "\n",
    "df = pd.read_csv('df_cleaned.csv')\n",
    "df\n",
    "\n",
    "# value_counts = df['CVD'].value_counts()\n",
    "# value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f01791-d9af-4d0c-bbbb-9d0ddd57ad34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Diastolic blood pressure', 'Systolic blood pressure', 'Age',\n",
       "       'Cholesterol', 'HDL cholesterol', 'LDL cholesterol', 'Triglycerides',\n",
       "       'Glucose', 'Glycated haemoglobin', 'Childhood trauma score',\n",
       "       'Depressive symptoms score', 'Anxiety symptoms score',\n",
       "       'Physically abused by family as a child',\n",
       "       'Felt hated by family member as a child',\n",
       "       'Sexually molested as a child',\n",
       "       'Someone to take to doctor when needed as a child', 'Felt loved',\n",
       "       'Hypertension', 'Smoking status', 'Physical activity', 'Depressed mood',\n",
       "       'Anhedonia', 'Appetite changes', 'Sleep problems',\n",
       "       'Psychomotor changes', 'Fatigue', 'Feelings of inadequacy',\n",
       "       'Cognitive problems', 'Suicidal ideation', 'Anxiety', 'Restlessness',\n",
       "       'Lack of relaxation', 'Concentration problems', 'Irritability',\n",
       "       'Antidepressant use', 'Diabetes', 'Gender', 'CVD Family history',\n",
       "       'CVD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05eb960f-dce5-4452-856d-21d54a5e3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, original_df, target_case_count=10000, target_control_count=10000,\n",
    "                 ordinal_columns=None, categorical_columns=None, numerical_columns=None,\n",
    "                 latent_dim=100, generator_optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                 discriminator_optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                 epochs=1000, batch_size=32):\n",
    "        \n",
    "        self.original_df = original_df.copy()\n",
    "        self.target_case_count = target_case_count\n",
    "        self.target_control_count = target_control_count\n",
    "        self.ordinal_columns = ordinal_columns or []\n",
    "        self.categorical_columns = categorical_columns or []\n",
    "        self.numerical_columns = numerical_columns or []\n",
    "        self.latent_dim = latent_dim\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Preprocessing tools\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.ordinal_encoder = OrdinalEncoder()\n",
    "        self.categorical_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "        # Models\n",
    "        self.generator = None\n",
    "        self.discriminator = None\n",
    "\n",
    "        # Save fitted encoders and column names\n",
    "        self.encoded_cat_columns = []\n",
    "\n",
    "    def balance_data(self):\n",
    "        cases = self.original_df[self.original_df['CVD'] == 1]\n",
    "        controls = self.original_df[self.original_df['CVD'] == 0]\n",
    "\n",
    "        print(f\"Original cases: {len(cases)}\")\n",
    "        print(f\"Original controls: {len(controls)}\")\n",
    "\n",
    "        undersampled_controls = controls.sample(\n",
    "            n=self.target_control_count, replace=False, random_state=42)\n",
    "\n",
    "        num_synthetic_cases = max(0, self.target_case_count - len(cases))\n",
    "        print(f\"Generating {num_synthetic_cases} synthetic cases\")\n",
    "\n",
    "        features = cases.drop(columns=['CVD'])\n",
    "        processed_features = self._preprocess_features(features, fit=True)\n",
    "\n",
    "        self._initialize_gan(processed_features)\n",
    "        self._train_gan(processed_features.astype(np.float32))\n",
    "\n",
    "        synthetic_data = self._generate_synthetic_data(num_synthetic_cases)\n",
    "        synthetic_df = self._postprocess_features(synthetic_data)\n",
    "        synthetic_df['CVD'] = 1\n",
    "\n",
    "        balanced_df = pd.concat([cases, synthetic_df, undersampled_controls], ignore_index=True)\n",
    "        balanced_df = shuffle(balanced_df, random_state=42)\n",
    "\n",
    "        return balanced_df, cases.reset_index(drop=True), synthetic_df.reset_index(drop=True)\n",
    "\n",
    "    def _preprocess_features(self, df, fit=False):\n",
    "        df = df.copy()\n",
    "    \n",
    "        if self.categorical_columns:\n",
    "            # Check all expected categorical columns are present\n",
    "            missing = [col for col in self.categorical_columns if col not in df.columns]\n",
    "            if missing:\n",
    "                raise ValueError(f\"Missing categorical columns: {missing}\")\n",
    "    \n",
    "            if fit:\n",
    "                self.categorical_encoder.fit(df[self.categorical_columns])\n",
    "                self.encoded_cat_columns = self.categorical_encoder.get_feature_names_out(self.categorical_columns)\n",
    "    \n",
    "            encoded_array = self.categorical_encoder.transform(df[self.categorical_columns])\n",
    "    \n",
    "            # Convert to dense if sparse matrix (OneHotEncoder may return sparse)\n",
    "            if hasattr(encoded_array, \"toarray\"):\n",
    "                encoded_array = encoded_array.toarray()\n",
    "    \n",
    "            # Check shape before creating DataFrame\n",
    "            actual_shape = encoded_array.shape[1]\n",
    "            expected_shape = len(self.encoded_cat_columns)\n",
    "    \n",
    "            if actual_shape != expected_shape:\n",
    "                raise ValueError(\n",
    "                    f\"Mismatch in encoded shape: got {actual_shape}, expected {expected_shape}.\\n\"\n",
    "                    f\"Most likely cause: some categories seen during fit() are missing in current data.\"\n",
    "                )\n",
    "    \n",
    "            encoded_df = pd.DataFrame(encoded_array, columns=self.encoded_cat_columns, index=df.index)\n",
    "    \n",
    "            df = df.drop(columns=self.categorical_columns)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "        if self.numerical_columns:\n",
    "            if fit:\n",
    "                self.scaler.fit(df[self.numerical_columns])\n",
    "            df[self.numerical_columns] = self.scaler.transform(df[self.numerical_columns])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _postprocess_features(self, generated_data):\n",
    "        df = pd.DataFrame(generated_data, columns=self._get_full_column_order())\n",
    "        out_df = pd.DataFrame()\n",
    "\n",
    "        if self.numerical_columns:\n",
    "            out_df[self.numerical_columns] = self.scaler.inverse_transform(df[self.numerical_columns])\n",
    "\n",
    "        if self.ordinal_columns:\n",
    "            out_df[self.ordinal_columns] = np.round(df[self.ordinal_columns]).astype(int)\n",
    "            for col in self.ordinal_columns:\n",
    "                min_val = int(self.original_df[col].min())\n",
    "                max_val = int(self.original_df[col].max())\n",
    "                out_df[col] = out_df[col].clip(min_val, max_val)\n",
    "\n",
    "        if self.categorical_columns:\n",
    "            cat_data = df[self.encoded_cat_columns].values\n",
    "            decoded = self.categorical_encoder.inverse_transform(cat_data)\n",
    "            out_df[self.categorical_columns] = decoded\n",
    "\n",
    "        return out_df\n",
    "\n",
    "    def _get_full_column_order(self):\n",
    "        return (\n",
    "            self.numerical_columns +\n",
    "            self.ordinal_columns +\n",
    "            list(self.encoded_cat_columns)\n",
    "        )\n",
    "\n",
    "    def _initialize_gan(self, processed_features):\n",
    "        input_dim = processed_features.shape[1]\n",
    "        self.generator = self._build_generator(input_dim)\n",
    "        self.discriminator = self._build_discriminator(input_dim)\n",
    "\n",
    "    def _build_generator(self, output_dim):\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(self.latent_dim,)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dense(output_dim, activation='linear')\n",
    "        ])\n",
    "\n",
    "    def _build_discriminator(self, input_dim):\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(input_dim,)),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')  # Use sigmoid since from_logits=False\n",
    "        ])\n",
    "\n",
    "    def _train_gan(self, real_data):\n",
    "        cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(real_samples):\n",
    "            noise = tf.random.normal([real_samples.shape[0], self.latent_dim])\n",
    "\n",
    "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                fake_samples = self.generator(noise, training=True)\n",
    "\n",
    "                real_output = self.discriminator(real_samples, training=True)\n",
    "                fake_output = self.discriminator(fake_samples, training=True)\n",
    "\n",
    "                gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "                disc_loss = (\n",
    "                    cross_entropy(tf.ones_like(real_output), real_output) +\n",
    "                    cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "                )\n",
    "\n",
    "            gradients_gen = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "            gradients_disc = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "            self.generator_optimizer.apply_gradients(zip(gradients_gen, self.generator.trainable_variables))\n",
    "            self.discriminator_optimizer.apply_gradients(zip(gradients_disc, self.discriminator.trainable_variables))\n",
    "\n",
    "        print(f\"Training GAN for {self.epochs} epochs...\")\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(0, real_data.shape[0], self.batch_size):\n",
    "                batch = real_data[i:i+self.batch_size]\n",
    "                train_step(batch)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{self.epochs} completed\")\n",
    "\n",
    "    def _generate_synthetic_data(self, num_samples):\n",
    "        noise = tf.random.normal([num_samples, self.latent_dim])\n",
    "        generated = self.generator(noise, training=False).numpy()\n",
    "        return generated\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('df_cleaned.csv')\n",
    "    ordinal_cols = [\n",
    "        'Childhood trauma score',\n",
    "        'Depressive symptoms score', 'Anxiety symptoms score',\n",
    "        'Physically abused by family as a child',\n",
    "        'Felt hated by family member as a child',\n",
    "        'Sexually molested as a child',\n",
    "        'Someone to take to doctor when needed as a child',\n",
    "        'Felt loved',\n",
    "        'Hypertension', 'Smoking status', 'Physical activity'\n",
    "    ]\n",
    "    \n",
    "    categorical_cols = [\n",
    "        'Depressed mood', 'Anhedonia', 'Appetite changes', 'Sleep problems',\n",
    "        'Psychomotor changes', 'Fatigue', 'Feelings of inadequacy',\n",
    "        'Cognitive problems', 'Suicidal ideation', 'Anxiety', 'Restlessness',\n",
    "        'Lack of relaxation',  'Concentration problems',\n",
    "        'Irritability', \n",
    "        'Antidepressant use', 'Diabetes', 'Gender',\n",
    "        'CVD Family history'\n",
    "    ]\n",
    "    \n",
    "    numerical_cols = [\n",
    "        'Diastolic blood pressure', 'Systolic blood pressure',\n",
    "        'Age', 'Cholesterol', 'HDL cholesterol', 'LDL cholesterol', \n",
    "        'Triglycerides', 'Glucose', 'Glycated haemoglobin'\n",
    "    ]\n",
    "\n",
    "    gen = DataGenerator(\n",
    "        original_df=df,\n",
    "        target_case_count=5000,\n",
    "        target_control_count=5000,\n",
    "        ordinal_columns=ordinal_cols,\n",
    "        categorical_columns=categorical_cols,\n",
    "        numerical_columns=numerical_cols,\n",
    "        epochs=1000,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    balanced_df, real_cases, synthetic_cases = gen.balance_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6deb8d3c-d610-460d-98e1-399e772cca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "from IPython.display import display\n",
    "\n",
    "class DataInspector:\n",
    "    def __init__(self):\n",
    "        self.results = pd.DataFrame(columns=['Variable', 'Type', 'Test', 'Statistic', 'P-value', 'Significant'])\n",
    "        self.figures = []\n",
    "\n",
    "    def evaluate_statistical_similarity(self, real_df, synthetic_df, numerical_cols, ordinal_cols, categorical_cols):\n",
    "        \"\"\"Main evaluation function that generates both table and plots\"\"\"\n",
    "        # Reset results for multiple runs\n",
    "        self.results = pd.DataFrame(columns=['Variable', 'Type', 'Test', 'Statistic', 'P-value', 'Significant'])\n",
    "        self.figures = []\n",
    "        \n",
    "        # Combine all columns for plotting\n",
    "        all_cols = numerical_cols + ordinal_cols + categorical_cols\n",
    "        \n",
    "        # Create figure for all distributions\n",
    "        self._create_kde_subplots(real_df, synthetic_df, all_cols)\n",
    "        \n",
    "        # Evaluate each variable type\n",
    "        if numerical_cols:\n",
    "            self._evaluate_numerical(real_df, synthetic_df, numerical_cols)\n",
    "        if ordinal_cols:\n",
    "            self._evaluate_ordinal(real_df, synthetic_df, ordinal_cols)\n",
    "        if categorical_cols:\n",
    "            self._evaluate_categorical(real_df, synthetic_df, categorical_cols)\n",
    "        \n",
    "        # Display results\n",
    "        #self._display_results()\n",
    "        \n",
    "    def _create_kde_subplots(self, real_df, synthetic_df, all_cols):\n",
    "        \"\"\"Create a grid of KDE plots comparing real and synthetic for each variable\"\"\"\n",
    "        if not all_cols:\n",
    "            return\n",
    "            \n",
    "        n_cols = 3  # Number of columns in the subplot grid\n",
    "        n_rows = int(np.ceil(len(all_cols) / n_cols))\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        if n_rows > 1:\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            axes = [axes] if isinstance(axes, plt.Axes) else axes\n",
    "        \n",
    "        for idx, col in enumerate(all_cols):\n",
    "            if col not in real_df.columns or col not in synthetic_df.columns:\n",
    "                continue\n",
    "                \n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Plot KDE for both real and synthetic data\n",
    "            sns.kdeplot(real_df[col], label='Real', ax=ax, fill=True, alpha=0.3, color='blue')\n",
    "            sns.kdeplot(synthetic_df[col], label='Synthetic', ax=ax, fill=True, alpha=0.3, color='orange')\n",
    "            \n",
    "            ax.set_title(col)\n",
    "            ax.set_xlabel('')\n",
    "            ax.legend()\n",
    "        \n",
    "        # Hide any empty subplots\n",
    "        for idx in range(len(all_cols), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        self.figures.append(fig)\n",
    "        \n",
    "    def _evaluate_numerical(self, real_df, synthetic_df, numerical_cols):\n",
    "        \"\"\"Evaluate numerical variables with KS test\"\"\"\n",
    "        for col in numerical_cols:\n",
    "            if col not in real_df.columns or col not in synthetic_df.columns:\n",
    "                continue\n",
    "                \n",
    "            # KS Test\n",
    "            ks_stat, p_value = stats.kstest(real_df[col].dropna(), \n",
    "                                         synthetic_df[col].dropna())\n",
    "            \n",
    "            # Add to results table\n",
    "            self.results.loc[len(self.results)] = {\n",
    "                'Variable': col,\n",
    "                'Type': 'Numerical',\n",
    "                'Test': 'KS Test',\n",
    "                'Statistic': ks_stat,\n",
    "                'P-value': p_value,\n",
    "                'Significant': p_value < 0.05\n",
    "            }\n",
    "\n",
    "    def _evaluate_ordinal(self, real_df, synthetic_df, ordinal_cols):\n",
    "        \"\"\"Evaluate ordinal variables with Mann-Whitney U test\"\"\"\n",
    "        for col in ordinal_cols:\n",
    "            if col not in real_df.columns or col not in synthetic_df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Mann-Whitney U Test\n",
    "            stat, p_value = mannwhitneyu(real_df[col].dropna(),\n",
    "                                      synthetic_df[col].dropna())\n",
    "            \n",
    "            # Add to results table\n",
    "            self.results.loc[len(self.results)] = {\n",
    "                'Variable': col,\n",
    "                'Type': 'Ordinal',\n",
    "                'Test': 'Mann-Whitney U',\n",
    "                'Statistic': stat,\n",
    "                'P-value': p_value,\n",
    "                'Significant': p_value < 0.05\n",
    "            }\n",
    "\n",
    "    def _evaluate_categorical(self, real_df, synthetic_df, categorical_cols):\n",
    "        \"\"\"Evaluate categorical variables with Chi-Square test\"\"\"\n",
    "        for col in categorical_cols:\n",
    "            if col not in real_df.columns or col not in synthetic_df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Prepare contingency table\n",
    "            real_counts = real_df[col].value_counts()\n",
    "            syn_counts = synthetic_df[col].value_counts()\n",
    "            all_cats = list(set(real_counts.index) | set(syn_counts.index))\n",
    "            contingency = pd.DataFrame({\n",
    "                'Real': real_counts.reindex(all_cats, fill_value=0),\n",
    "                'Synthetic': syn_counts.reindex(all_cats, fill_value=0)\n",
    "            })\n",
    "            \n",
    "            # Chi-Square Test\n",
    "            chi2, p_value, _, _ = chi2_contingency(contingency)\n",
    "            \n",
    "            # Add to results table\n",
    "            self.results.loc[len(self.results)] = {\n",
    "                'Variable': col,\n",
    "                'Type': 'Categorical',\n",
    "                'Test': 'Chi-Square',\n",
    "                'Statistic': chi2,\n",
    "                'P-value': p_value,\n",
    "                'Significant': p_value < 0.05\n",
    "            }\n",
    "\n",
    "    def _display_results(self):\n",
    "        \"\"\"Display all results in organized format\"\"\"\n",
    "        # Create a display copy with formatted numbers\n",
    "        display_df = self.results.copy()\n",
    "        display_df['Statistic'] = display_df['Statistic'].apply(lambda x: f\"{x:.4f}\" if isinstance(x, (int, float)) else x)\n",
    "        display_df['P-value'] = display_df['P-value'].apply(lambda x: f\"{x:.4f}\" if isinstance(x, (int, float)) else x)\n",
    "        \n",
    "        # Display results table\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Statistical Test Results Summary\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        def highlight_significant(row):\n",
    "            return ['background-color: #ffcccc' if row['Significant'] else '' for _ in row]\n",
    "        \n",
    "        display(display_df.style.apply(highlight_significant, axis=1))\n",
    "\n",
    "# Usage remains the same\n",
    "inspector = DataInspector()\n",
    "inspector.evaluate_statistical_similarity(\n",
    "    real_df, \n",
    "    synthetic_df,\n",
    "    numerical_cols=numerical_cols,\n",
    "    ordinal_cols=ordinal_cols,\n",
    "    categorical_cols=categorical_cols\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
