{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2106d12",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6cc50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import  KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,roc_curve,auc \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report, f1_score, average_precision_score, precision_recall_fscore_support)\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import statistics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import interp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,roc_curve,auc \n",
    "from sklearn.metrics import recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4809f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('tranformLuric.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef97f2",
   "metadata": {},
   "source": [
    "Inspect data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "288caa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate(df,col,vartype,hue =None):    \n",
    "    '''\n",
    "    Univariate function will plot parameter values in graphs.\n",
    "    df      : dataframe name\n",
    "    col     : Column name\n",
    "    vartype : variable type : continuous or categorical\n",
    "                Continuous(0)   : Distribution, Violin & Boxplot will be plotted.\n",
    "                Categorical(1) : Countplot will be plotted.\n",
    "    hue     : Only applicable in categorical analysis.\n",
    "    '''\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    if vartype == 0:\n",
    "        fig, ax=plt.subplots(nrows =1,ncols=3,figsize=(20,8))\n",
    "        ax[0].set_title(\"Distribution Plot\")\n",
    "        sns.distplot(df[col],ax=ax[0])\n",
    "        ax[1].set_title(\"Violin Plot\")\n",
    "        sns.violinplot(data =df, x=col,ax=ax[1], inner=\"quartile\")\n",
    "        ax[2].set_title(\"Box Plot\")\n",
    "        sns.boxplot(data =df, x=col,ax=ax[2],orient='v')\n",
    "    if vartype == 1:\n",
    "        temp = pd.Series(data = hue)\n",
    "        fig, ax = plt.subplots()\n",
    "        width = len(df[col].unique()) + 6 + 4*len(temp.unique())\n",
    "        fig.set_size_inches(width , 7)\n",
    "        ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue) \n",
    "        if len(temp.unique()) > 0:\n",
    "            for p in ax.patches:\n",
    "                ax.annotate('{:1.1f}%'.format((p.get_height()*100)/float(len(df))), (p.get_x()+0.05, p.get_height()+20))  \n",
    "        else:\n",
    "            for p in ax.patches:\n",
    "                ax.annotate(p.get_height(), (p.get_x()+0.32, p.get_height()+20)) \n",
    "        del temp\n",
    "    else:\n",
    "        exit\n",
    "    plt.show()\n",
    "    \n",
    "#univariate(df=raw_data,col='cadyn',vartype=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e37868",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b546e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_mean_center(x):\n",
    "    mu = np.mean(x)\n",
    "    xm = x/mu\n",
    "    return xm, mu\n",
    "\n",
    "def mean_center_transform(df, cols):\n",
    "    df_new = pd.DataFrame()\n",
    "    sc = {}\n",
    "    for col in cols:\n",
    "        x = df[col].values\n",
    "        df_new[col], mu = apply_mean_center(x)\n",
    "        sc[col] = mu\n",
    "    return df_new, sc\n",
    "\n",
    "def apply_standardize(x):\n",
    "    mu = np.mean(x)\n",
    "    sd = np.std(x)\n",
    "    xm = (x-mu)/sd\n",
    "    return xm, mu, sd\n",
    "\n",
    "def standardize_transform(df, cols):\n",
    "    df_new = pd.DataFrame()\n",
    "    mean = {}\n",
    "    sd = {}\n",
    "    for col in cols:\n",
    "        x = df[col].values\n",
    "        df_new[col], mu, std = apply_standardize(x)\n",
    "        mean[col] = mu\n",
    "        sd[col] = std\n",
    "    return df_new, mean, sd\n",
    "\n",
    "def apply_scale(x):\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    xm = (x-x_min)/(x_max-x_min)\n",
    "    return xm, x_min, x_max\n",
    "\n",
    "def scale_transform(df, cols):\n",
    "    df_new = pd.DataFrame()\n",
    "    min_dict = {}\n",
    "    max_dict = {}\n",
    "    for col in cols:\n",
    "        x = df[col].values\n",
    "        df_new[col], x_min, x_max = apply_scale(x)\n",
    "        min_dict[col] = x_min\n",
    "        max_dict[col] = x_max\n",
    "    return df_new, min_dict, max_dict\n",
    "def float_to_int (df):\n",
    "    float_col = df.select_dtypes(include=('string'))\n",
    "    for col in float_col.columns.values:\n",
    "         raw_data[col] = raw_data[col].astype('float64')\n",
    "    return raw_data\n",
    "raw_data = float_to_int(raw_data)\n",
    "for col in raw_data.columns:\n",
    "      raw_data[col] = pd.to_numeric(raw_data[col],errors = 'coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378fc56",
   "metadata": {},
   "source": [
    "Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f6a3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df, threshold=3):\n",
    "    \"\"\"\n",
    "    Replace outliers with NaN values for each column separately\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - threshold*iqr\n",
    "        upper_bound = q3 + threshold*iqr\n",
    "        df_out[col] = np.where((df[col] < lower_bound) | (df[col] > upper_bound), np.nan, df[col])\n",
    "    return df_out\n",
    "\n",
    "\n",
    "data_without_outliers = detect_outliers_zscore(raw_data)\n",
    "\n",
    "def detect_outliers_zscore(df, threshold=3):\n",
    "    \"\"\"\n",
    "    Replace outliers with NaN values for each column separately using Z-score approach\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        zscores = (df[col] - df[col].mean()) / df[col].std()\n",
    "        df_out[col] = np.where(np.abs(zscores) > threshold, np.nan, df[col])\n",
    "    return df_out\n",
    "\n",
    "\n",
    "data_without_outliers_zscore = detect_outliers_zscore(raw_data)\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def detect_outliers_lof(df, n_neighbors=20, contamination=0.1):\n",
    "    \"\"\"\n",
    "    Replace outliers with NaN values using LOF approach for each column separately\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        clf = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "        outlier_scores = clf.fit_predict(df[col].values.reshape(-1, 1))\n",
    "        df_out[col] = np.where(outlier_scores == -1, np.nan, df[col])\n",
    "    return df_out\n",
    "\n",
    "data_without_outliers_LOF = detect_outliers_lof(raw_data)\n",
    "# Print the number of missing values in each column\n",
    "#print(data_without_outliers_LOF.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986c309b",
   "metadata": {},
   "source": [
    "Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7437b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(df):\n",
    "    percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "    missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "    \n",
    "    # Drop columns with >= 25 % of missing values \n",
    "    to_keep = (missing_value_df[missing_value_df.iloc[:,-1]<=25.0].index)\n",
    "    df_new = df.drop(df.columns.difference(to_keep), axis=1)\n",
    "\n",
    "    return [missing_value_df, df_new]\n",
    "\n",
    "def impute_mixed_var_new(df_data, rv_type):\n",
    "    imp_knn = KNNImputer(n_neighbors=2)\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp_med = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    imp_freq = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    for rvs in list(df_data):\n",
    "        data_impute = imp_mean.fit_transform(pd.DataFrame(df_data[rvs]))\n",
    "        data_impute = imp_knn.fit_transform(pd.DataFrame(df_data[rvs]))\n",
    "        if rv_type[rvs] == 'g':\n",
    "            # data_impute = imp_knn.fit_transform(pd.DataFrame(df_data[rvs]))\n",
    "            data_impute = imp_mean.fit_transform(pd.DataFrame(df_data[rvs]))\n",
    "        elif rv_type[rvs] == 'p':\n",
    "            data_impute = imp_med.fit_transform(pd.DataFrame(df_data[rvs]))\n",
    "        else:\n",
    "            data_impute = imp_freq.fit_transform(pd.DataFrame(df_data[rvs]))\n",
    "        df_data[rvs] = data_impute\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b697a98",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature matrix X and the target vector y\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe98c5",
   "metadata": {},
   "source": [
    "Feature Selection (Include results from R code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca925c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from skrebate import ReliefF\n",
    "\n",
    "# Compute the correlation coefficient and p-value between each feature and the target\n",
    "correlations, p_values = [], []\n",
    "for feature in range(X.shape[1]):\n",
    "    corr, p_val = pearsonr(X[:, feature], y)\n",
    "    correlations.append(abs(corr))\n",
    "    p_values.append(p_val)\n",
    "\n",
    "# Select the top K features with the highest correlation coefficient\n",
    "K = 5\n",
    "selected_features = df.drop('target', axis=1).columns[sorted(range(X.shape[1]), key=lambda i: correlations[i], reverse=True)[:K]]\n",
    "\n",
    "# Subset the data with the selected features\n",
    "X_new = df[selected_features].values\n",
    "\n",
    "# Compute the mutual information between each feature and the target\n",
    "mi = mutual_info_classif(X, y)\n",
    "\n",
    "# Select the top K features with the highest mutual information\n",
    "K = 5\n",
    "selector = SelectKBest(mutual_info_classif, k=K)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Compute the feature importance using ReliefF\n",
    "estimator = KNeighborsClassifier(n_neighbors=5)\n",
    "fs = ReliefF(n_features_to_select=K, n_neighbors=100, estimator=estimator)\n",
    "fs.fit(X, y)\n",
    "importances = fs.feature_importances_\n",
    "\n",
    "# Select the top K features with the highest feature importance\n",
    "selected_features = df.drop('target', axis=1).columns[importances.argsort()[::-1][:K]]\n",
    "X_new = df[selected_features].values\n",
    "\n",
    "#######################################################################################\n",
    "def backward_elimination(data, target,significance_level = 0.05):\n",
    "    features = data.columns.tolist()\n",
    "    while(len(features)>0):\n",
    "        features_with_constant = sm.add_constant(data[features])\n",
    "        p_values = sm.OLS(target, features_with_constant.astype(float)).fit().pvalues[1:]\n",
    "        max_p_value = p_values.max()\n",
    "        if(max_p_value >= significance_level):\n",
    "            excluded_feature = p_values.idxmax()\n",
    "            features.remove(excluded_feature)\n",
    "        else:\n",
    "            break \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736d78f",
   "metadata": {},
   "source": [
    "Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89460a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Create correlation matrix\n",
    "corr = data.corr()\n",
    "# Plot heatmap and Remove one of the correlated variables: If two or more variables are highly correlated, we can remove one of them to reduce the multicollinearity. \n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "#############################################################################################\n",
    "#VIF (Variance Inflation Factor) is another commonly used method for detecting multicollinearity\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Drop the target variable and categorical variables, if any\n",
    "X = data.drop(['target', 'categorical_var'], axis=1)\n",
    "\n",
    "# Calculate VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"features\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif)\n",
    "\n",
    "###########################################################################################\n",
    "#Use regularization: Regularization techniques, such as ridge regression \n",
    "#and lasso regression, can be used to reduce the impact of multicollinearity. \n",
    "#These techniques add a penalty term to the regression equation, \n",
    "#which helps to reduce the coefficients of highly correlated variables.\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create Ridge regression object\n",
    "ridge = Ridge(alpha=0.5)\n",
    "\n",
    "# Fit model\n",
    "ridge.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10834ab0",
   "metadata": {},
   "source": [
    "Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd2bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import raw data\n",
    "\n",
    "cols = ['age','sex','eapoa1','Troponin_T','smoclass','elpa','ace','haptoglo','cadyn']\n",
    "data = raw_data[cols]\n",
    "\n",
    "x_new= data.drop(['cadyn'],axis=1)\n",
    "y_new= data.iloc[:,-1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, y_test, y_score, yhat):\n",
    "        self.y_test = y_test\n",
    "        self.y_score = y_score\n",
    "        self.yhat = yhat\n",
    "        \n",
    "    def accuracy(self):\n",
    "        return accuracy_score(self.y_test, self.y_score)\n",
    "    \n",
    "    def sensitivity(self):\n",
    "        return recall_score(self.y_test, self.y_score, pos_label=1)\n",
    "    \n",
    "    def specificity(self):\n",
    "        tn, fp, fn, tp = confusion_matrix(self.y_test, self.y_score).ravel()\n",
    "        return tn / (tn + fp)\n",
    "    \n",
    "    def roc_auc(self):\n",
    "        fpr, tpr, thresholds = roc_curve(self.y_test, self.yhat)\n",
    "        return auc(fpr, tpr)\n",
    "    \n",
    "    def recall(self):\n",
    "        return recall_score(self.y_test, self.y_score, average='macro')\n",
    "    \n",
    "    def precision(self):\n",
    "        return precision_score(self.y_test, self.y_score, zero_division=0)\n",
    "\n",
    "def run_classification(model, x_new, y_new):\n",
    "    accs, senss, specs, tprs, aucs = [], [], [], [], []\n",
    "    cms = np.zeros((2, 2)) # initialize empty confusion matrix\n",
    "    cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\n",
    "    for train_index, test_index in cv_outer.split(x_new, y_new):\n",
    "        x_train, x_test = x_new.iloc[train_index], x_new.iloc[test_index]\n",
    "        y_train, y_test = y_new.iloc[train_index], y_new.iloc[test_index]\n",
    "        model.fit(x_train, y_train)\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(x_test)\n",
    "        expected_value = explainer.expected_value\n",
    "        yhat = model.predict_proba(x_test)[:, 1]\n",
    "        y_score = (yhat > 0.5) * 1\n",
    "        metrics = Metrics(y_test, y_score, yhat)\n",
    "        accs.append(metrics.accuracy())\n",
    "        senss.append(metrics.sensitivity())\n",
    "        specs.append(metrics.specificity())\n",
    "        tprs.append(np.interp(mean_fpr, metrics.fpr(), metrics.tpr()))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = metrics.roc_auc()\n",
    "        aucs.append(roc_auc)\n",
    "        cm = confusion_matrix(y_test, y_score)\n",
    "        cms += cm\n",
    "        # Generate summary plot for each fold\n",
    "        shap.summary_plot(shap_values, x_test, title=\"SHAP summary plot for fold\")\n",
    "    avg_cm = cms / 10 # calculate average confusion matrix\n",
    "    return (np.mean(accs), np.mean(senss), np.mean(specs), np.mean(aucs), np.mean(tprs, axis=0), avg_cm)\n",
    "\n",
    "\n",
    "# Define the models to be tested\n",
    "classifiers = [\n",
    "    RandomForestClassifier(bootstrap=True, max_depth=100, max_features=2, min_samples_leaf=5, min_samples_split=10, n_estimators=100),\n",
    "    GradientBoostingClassifier(criterion='friedman_mse', loss='exponential', min_samples_leaf=3, min_samples_split=2, n_estimators=150),\n",
    "]\n",
    "\n",
    "# Perform classification and store results\n",
    "results = []\n",
    "for model in classifiers:\n",
    "    print(\"Model:\", model)\n",
    "    results.append(run_classification(model, x_new, y_new))\n",
    "\n",
    "# Display results\n",
    "for i, model in enumerate(classifiers):\n",
    "    print(type(model).__name__)\n",
    "    print('Accuracy:', results[i][0])\n",
    "    print('Sensitivity:', results[i][1])\n",
    "    print('Specificity:', results[i][2])\n",
    "    print('AUC:', results[i][3])\n",
    "    print('Average Confusion Matrix:')\n",
    "    print(results[i][5])\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "# Generate summary bar plot for each model\n",
    "for i, model in enumerate(classifiers):\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(x_new)\n",
    "    expected_value = explainer.expected_value\n",
    "    shap.summary_plot(shap_values, x_new, plot_type=\"bar\", title=\"SHAP summary plot for {}\".format(type(model).__name__))\n",
    "\n",
    "    \n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--') \n",
    "for i, model in enumerate(classifiers):\n",
    "    mean_tprs = results[i][4]\n",
    "    mean_tprs[-1] = 1.\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    plt.plot(mean_fpr, mean_tprs, label='{} (AUC = {:.2f})'.format(type(model).__name__, results[i][3]))\n",
    "    \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
